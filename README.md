# EMSE2020Data
Data for EMSE 2020 journal: Using Black-Box Performance Models to Detect Performance Regressions under Varying Workloads: An Empirical Study

There are three folders in this package, `SubjectData`, `Models&Scripts`, and `Results`.

### SubjecData folder

- `Data`  contains the logs and performnce data comes from performance testing on two open-source subject projects: OpenMRS and Apache James.
- `LoadTestDriver` includes the used Jmeter load testing scripts for open-source subject systems used in our paper (i.e., OpenMRS and Apache James) and they are in the corresponding zip package. Each package includes 5 different scripts which represents five different workloads.
  - `OpenMRS` the version we used in our paper is `2.1.4`, practioners should have the system under test (SUT) deployed on you prefered servers. 
    - Each `.jmx` file represents a workload. We designed different performance tests that are composed of eight various test actions, including 1) creation of patients, 2) deletion of patients, 3) searching for patients, 4) editing patients, 5) searching for concepts, 6) searching for encounters, 7) searching for observations, and 8) searching for types of encounters. We use the first four workloads as `4w` and all five workloads as `5w`. 
    - Recommond experiment environments for the SUT: We deployed the OpenMRS on two machines, each with Intel Core i5-2400 CPU (3.10GHz), 8 GB memory, 512GB SATA hard drive. One machine is deployed as the application server and another machine is deployed as MySQL database server
    - Recommond experiment environments for the Jmeter: We used the RESTFul API from OpenMRS and ran JMeter on five extra machines with the same specification to simulate users on the client side with a five-hour workload.
    - You should use the performance monitoroing tool (e.g., psutil, pidstat) to genereate the performance metrics.
    - Once you finised runnign the load testing, you will get the access log files `(e.g., xxxx-xx-xx-weblog.log)` in the log folder under your web server (diffenert web servers may vary).
  - `Apache James` the version we used in our paper is `2.3.2`, `3.0M1`, and `3.0M2`, practioners should have the system under test (SUT) deployed on you prefered servers. 
    - Each `.jmx` file represents a workload. In total, we built eight actions for the performance test, including 1) sending mails with short messages and without attachments, 2) sending mails with long messages and without attachments, 3) sending mails with short messages and with small attachments, 4) sending mails with short messages and with large attachments, 5) sending mails with long messages and with small attachments, 6) sending mails with long messages and with large attachments, 7) retrieving entire mails, and 8) retrieving only the header of mails. We use the first four workloads as `4w` and all five workloads as `5w`. 
    - Recommond experiment environments for the SUT: We deployed Apache James on a server machine with an Intel Core i7-8700K CPU (3.70GHz), 16 GB memory on a 3TB SATA hard drive.
    - Recommond experiment environments for the Jmeter: We ran JMeter on five extra machines with Intel Core i5-2400 CPU (3.10GHz), 8 GB memory and 320GB SATA hard drive to generate multiple five-hour workloads. 
    - You should use the performance monitoroing tool (e.g., psutil, pidstat) to genereate the performance metrics.
    - Once you finised running the load testing, you will get the access log files in the Jmeter machine, which is called `actionlog.jtl`. (P.S. For the problem of broken entries in the file actionlog.jtl, please change its suffix to “.csv” (e.g., actionlog.csv), then it will show one log entry in one line. In this file, each row represents a client request to the system. Although each row may contain many attributes, we only focus on the request event and its corresponding timestamp. )

### Models&Scripts folder

- `dl&ml` contains  the scripts about building the black-box performance model used in our research. For linear regression, random forest, and XGBoost, the input of the models are vectors whose values are the number of appearances of each log event in a time period. For CNN, RNN and LSTM, we sort the logs in each time period by their corresponding time stamp to create a sequence as the input of the neural networks.
- `processingdata`  comprises the scripts to extract log metrics and performance metrics to build black-box performace models. The input of procesing data should contians at least two files. One is the perofmrnace data generated by your performance monitoroing tool (e.g., psutil, pidstat), another file is the access logs that are often automaticallly generated by your web server. Since OpenMRS and Aapche James have different formats of web access logs, we write the didfferent data processing sciprts for those two subject systems and they are in corresponding packages. The output should be one spreedsheet which contains both log metrics and performance metrics. The format of the output data should be exactly like Table 1 in our paper. Prctioners can modify the existing scripts or rewirite your own data processing scripts if you want apply our aporoach on your target subject systems. 

### Results folder

- `RQ1`  contains the experiment data and graphs for the first research question (RQ1):
  -  `prediction_error.zip` is the zip file that includes the raw performance prdiciton errors data of six performance models used in our study (i.e., CNN, RNN, LSTM, Linear Regression, Random Forest, and XGBoost) for both OpenMRS and Apache James System.
  - `violincharts.zip` comprises the violin charts of the performance prdiciton errors in the aboving zip file which present its distributions in a more intuitive and readable way.
- `RQ2` contains the spreedsheet file that shows the reuslts of three approaches proposed in our paper of performance regression detection on OpenMRS and Apache James (RQ2).

